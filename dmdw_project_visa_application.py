# -*- coding: utf-8 -*-
"""DMDW Project - Visa Application.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14V0QKg-iekLpdHwGQukwu-WCLXOZqlPQ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib
# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn')

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Sem 7/DMDW Project /'

tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),
             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),
             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),
             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),
             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]

for i in range(len(tableau20)):
    r, g, b = tableau20[i]
    tableau20[i] = (r / 255., g / 255., b / 255.)

df = pd.read_csv(path+'project.csv', engine='python', encoding='utf-8',on_bad_lines='warn')
df = df.dropna(axis=0)
df.drop('Unnamed: 0',inplace=True ,axis=1)
data = df
df.head()

"""### Cleaning to remove duplicates with different cases"""

df['EMPLOYER_NAME'] = df['EMPLOYER_NAME'].apply(lambda x : x.upper())
df['SOC_NAME']= df['SOC_NAME'].apply(lambda x : x.title())
df['JOB_TITLE']= df['JOB_TITLE'].apply(lambda x : x.title())
df['FULL_TIME_POSITION'] = df['FULL_TIME_POSITION'].apply(lambda x : x.upper())
df['WORKSITE'] = df['WORKSITE'].apply(lambda x : x.title())

df.describe()

df.info(null_counts=True)

print("The shape of the dataset is : {}".format(df.shape))

print(f"There were around {df.shape[0]} applications for H-1B Visa from {df.YEAR.min()} to {df.YEAR.max()}.")

df.CASE_STATUS.value_counts()

"""## Exploratory Data Analysis EDA"""

plt.figure(figsize=(10,7))
df.CASE_STATUS.value_counts().plot(kind='barh',  color=tableau20)
df.sort_values('CASE_STATUS')
plt.title("NUMBER OF APPLICATIONS")
plt.show()

"""From the above graph we can say that the employees who have applied for the H-1B Visa were more than 2500000 whose application got certified and there were more than 200000 whose application's were certified and withdrawn and there were around 90000 whose application's were denied and there we around 80000 were withdrawn"""

df.YEAR.value_counts().plot(kind = 'bar',color=tableau20)

"""We can that there is an exponential increase in the number of applications as the year passes"""

plt.figure(figsize=(10,7))

ax1 = df['EMPLOYER_NAME'][df['YEAR'] == 2011].groupby(df['EMPLOYER_NAME']).count().sort_values(ascending=False).head(10).plot(kind='barh', title = "Top 10 Applicants in 2011",
                                                                                                                           color=tableau20)
ax1.set_label("")
plt.show()

plt.figure(figsize=(10,7))

ax2 = df['EMPLOYER_NAME'][df['YEAR'] == 2016].groupby(df['EMPLOYER_NAME']).count().sort_values(ascending=False).head(10).plot(kind='barh', title='Top 10 Applicants in 2016'
                                                                                                                             ,color=tableau20)
ax2.set_ylabel("")
plt.show()

plt.figure(figsize=(10,7))

ax3 = df['EMPLOYER_NAME'].groupby([df['EMPLOYER_NAME']]).count().sort_values(ascending=False).head(10).plot(kind = 'barh', title = 'Top 10 Applicants from 2011 to 2016'
                                                                                                           ,color=tableau20)
ax3.set_ylabel("")
plt.show()

top_emp = list(df['EMPLOYER_NAME'][df['YEAR'] >= 2015].groupby(df['EMPLOYER_NAME']).count().sort_values(ascending=False).head(10).index)

byempyear = df[['EMPLOYER_NAME', 'YEAR', 'PREVAILING_WAGE']][df['EMPLOYER_NAME'].isin(top_emp)]

byempyear = byempyear.groupby([df['EMPLOYER_NAME'], df['YEAR']])

plt.figure(figsize=(12,7))

markers=['o','v','^','<','>','d','s','p','*','h','x','D','o','v','^','<','>','d','s','p','*','h','x','D']

for company in top_emp:
    tmp = byempyear.count().loc[company]
    plt.plot(tmp.index.values, tmp["PREVAILING_WAGE"].values, label=company, linewidth=2,marker=markers[top_emp.index(company)])
plt.xlabel("Year")
plt.ylabel("Number of Applications")
plt.legend()
plt.title('Number of Applications of Top 10 Applicants')
plt.show()

"""* We can clearly see that there are 2 new companies which are TECH MAHINDRA(AMERICAS),INC. & CAPGEMINI AMERICA.
* INFOSYS showed rapid growth between the year 2011 and 2013 where it came from 0 applications to more than 30k applications.
* TATA also showed a significant growth.
* From the above plot except the 2 new comers we can say that the number of applications receving to the top 10 employer started decreasing from the year 2015.
* All very top applications are from India.
* These are the companies who filed the most number of applications.
"""

plt.figure(figsize=(12,7))

for company in top_emp:
    tmp = byempyear.mean().loc[company]
    plt.plot(tmp.index.values, tmp["PREVAILING_WAGE"].values, label=company, linewidth=2,marker=markers[top_emp.index(company)])
plt.xlabel("Year")
plt.ylabel("Average Salary offered (USD)")
plt.legend()
plt.title('Average Salary of Top 10 Applicants')
plt.show()

"""* We can see that the Average Salary offered by Infosys was very high as compared to rest of the companies in the year 2012.
* It's very interesting to see a huge peak in 2014 by IBM INDIA PRIVATE LIMITED looking like something went wrong.
* More sudden peak's were observed by ACCENTURE LLP in year 2011 and by TECH MAHINDRA in the year 2016.
"""

plt.figure(figsize=(10,12))
df.JOB_TITLE.value_counts().nlargest(20).plot(kind = 'barh', title = "Top 20 Job Titles",color=tableau20)
plt.show()

plt.figure(figsize=(12,7))
sns.set(style="whitegrid")
g = sns.countplot(x = 'FULL_TIME_POSITION', data = df)
plt.title("NUMBER OF APPLICATIONS MADE FOR THE FULL TIME POSITION")
plt.ylabel("NUMBER OF PETITIONS MADE")
plt.show()

"""From the countplot we can see that around 85% of the total jobs are full time."""

total_certified_cases = df[df['CASE_STATUS']=='CERTIFIED']
sns.boxplot(total_certified_cases['PREVAILING_WAGE'])

certified_cases = total_certified_cases[total_certified_cases['PREVAILING_WAGE']<150000] # 500000
sns.boxplot(certified_cases['PREVAILING_WAGE'])

plt.figure(figsize=(30,8))
sns.distplot(certified_cases['PREVAILING_WAGE'],bins=70,kde=True,hist_kws={"edgecolor": "black"})
plt.xlabel('PREVALING WAGES',size=20)
plt.ylabel('COUNT OF CERTIFIED CASES', size=20)
plt.title('WAGE DISTRIBUTION BETWEEN 2011 TO 2016',size=20)

top_twenty=certified_cases['EMPLOYER_NAME'].value_counts()[:20]
top_twenty.to_frame()

"""This represents as to who are the main benificaries of H1B. It is predominately dominated by Indian IT companies."""

plt.figure(figsize=(30,20))
sns.barplot(x=top_twenty.values,y=top_twenty.index,color='teal')
plt.xlabel('COUNT OF CERTIFIED CASES',size=20)
plt.ylabel('EMPLOYERS', size=20)
plt.title('TOP TWENTY BENIFICARIES 2011 TO 2016',size=20)
plt.show()

f,axes = plt.subplots(nrows=5,ncols=1, figsize=(10,10),sharey=False)
rowcount=0
companies = top_twenty.index[:5]
for company in companies:
    axes[rowcount].set_title(company)
    wage = certified_cases[certified_cases['EMPLOYER_NAME']==company]['PREVAILING_WAGE']
    sns.distplot(wage ,ax=axes[rowcount],bins=100,hist_kws= {"edgecolor":"#E6E6E6", "color": "#EE6666"}, kde=True)
    rowcount+=1

plt.tight_layout()

"""Taking the top 5 benificaries of H1B and see how their salary is distributed"""

top_roles = certified_cases['SOC_NAME'].value_counts().head(10)
plt.figure(figsize=(15,10))
sns.barplot(y = top_roles.index ,x = top_roles.values ,color='m')
plt.xlabel('COUNT OF CERTIFIED CASES',size=20)
plt.ylabel('JOB ROLES', size=20)
plt.title('TOP JOB BENIFICARIES 2011 TO 2016',size=20)

top_payers = certified_cases.groupby(by=['SOC_NAME'])['PREVAILING_WAGE'].mean()
top_payers = top_payers.sort_values(ascending=False)[:10]
plt.figure(figsize=(20,5))
sns.barplot(y = top_payers.index ,x = top_payers.values ,color='teal')
plt.xlabel('AVERAGE SALARY',size=20)
plt.ylabel('JOB ROLES', size=20)
plt.title('HIGH PAYING JOB BENIFICARIES 2011 TO 2016',size=20)
plt.xlim((120000,145000))

"""The 2 graphs demonstrates which are the top job benificaries and high paying jobs from 2011 to 2016"""

plt.figure(figsize=(10,10))
sns.boxplot(data=certified_cases, x='YEAR', y='PREVAILING_WAGE',hue='FULL_TIME_POSITION')
plt.tight_layout()

"""In 2016 partime employees earned significantly lower

## Feature Engineering
"""

## Checking for null values
df.isnull().sum()

"""**Label enconding the CASE_STATUS, FULL_TIME_POSITION, SOC_NAME feature**


---


What is Label Encoding ?

-

Label Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.
"""

df.CASE_STATUS.value_counts()

# df['CASE_STATUS'] = df['CASE_STATUS'].map({'CERTIFIED' : 0, 'CERTIFIED-WITHDRAWN' : 1, 'DENIED' : 2, 'WITHDRAWN' : 3,
#                                            'PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED' : 4, 'REJECTED' : 5, 'INVALIDATED' : 6})

df['CASE_STATUS'] = df['CASE_STATUS'].map({'CERTIFIED' : 0, 'CERTIFIED-WITHDRAWN' : 100, 'DENIED' : 1, 'WITHDRAWN' : 100,
                                           'PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED' : 100, 'REJECTED' : 1, 'INVALIDATED' : 1})

df = df[df['CASE_STATUS'] < 50]

df.CASE_STATUS.value_counts()

df.FULL_TIME_POSITION.value_counts()

df['FULL_TIME_POSITION'] = df['FULL_TIME_POSITION'].map({'N' : 0, 'Y' : 1})

df['SOC_NAME'].value_counts()

import sys
df['SOC_NAME_alt'] = 'others'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Computer')] = 'it'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Software')] = 'it'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Chief')] = 'manager'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Management')] = 'manager'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Mechanical')] = 'mechanical'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Database')] = 'database'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Sales')] = 'scm'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Market')] = 'scm'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Financial')] = 'finance'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Public')] = 'pr'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Fundraising')] = 'pr'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Education')] = 'administrative'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Law')] = 'administrative'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Auditors')] = 'audit'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Compliance')] = 'audit'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Distribution')] = 'scm'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Logistics')] = 'scm'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Recruiters')] = 'hr'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Human')] = 'hr'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Agricultural')] = 'agri'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Farm')] = 'agri'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Construction')] = 'estate'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Architectural')] = 'estate'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Forencsic')] = 'medical'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Health')] = 'medical'
df['SOC_NAME_alt'][df['SOC_NAME'].str.contains('Teachers')] = 'education'

df[['SOC_NAME','SOC_NAME_alt']]

def wage_categorization(wage):
    if wage <=50000:
        return "VERY LOW"
    elif wage >50000 and wage <= 70000:
        return "LOW"
    elif wage >70000 and wage <= 90000:
        return "MEDIUM"
    elif wage >90000 and wage<=150000:
        return "HIGH"
    elif wage >=150000:
        return "VERY HIGH"

df['WAGE_CATEGORY'] = df['PREVAILING_WAGE'].apply(wage_categorization)

def state_extractor(work_site):
    return work_site.split(', ')[1]

df['WORKSITE'] = df['WORKSITE'].apply(state_extractor)

df.head()

"""Dropping the unwanted columns from the dataset."""

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(df.SOC_NAME_alt)
df['SOC_NAME']=le.transform(df['SOC_NAME_alt'])
le.fit(df.WORKSITE)
df['WORKSITE']=le.transform(df['WORKSITE'])
le.fit(df.YEAR)
df['YEAR']=le.transform(df['YEAR'])
le.fit(df.WAGE_CATEGORY)
df['WAGE_CATEGORY']=le.transform(df['WAGE_CATEGORY'])

df = df.drop(['SOC_NAME_alt','JOB_TITLE','EMPLOYER_NAME','lon', 'lat', 'PREVAILING_WAGE','JOB_TITLE'], axis=1)

from sklearn.preprocessing import MinMaxScaler

# scale features
scaler = MinMaxScaler()
df[['SOC_NAME', 'FULL_TIME_POSITION','YEAR','WORKSITE','WAGE_CATEGORY']] = scaler.fit_transform(df[['SOC_NAME', 'FULL_TIME_POSITION','YEAR','WORKSITE','WAGE_CATEGORY']])

df.reset_index(drop=True, inplace=True)

df.describe()

df.reset_index(drop=True, inplace=True)

"""## Feature Selection
Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.
"""

sns.heatmap(df.corr(), annot=True, cmap="RdYlGn", annot_kws={"size":15})

df.CASE_STATUS.value_counts()

def sample_data(data, data_size):
  df_sampled = resample(data, replace=True, n_samples=data_size)
  return df_sampled

data_size = 30000
# df.sample(n = data_size)

df_0 = df[df.CASE_STATUS==0]
df_0 = sample_data(df_0, data_size)
df_1 = df[df.CASE_STATUS==1]
df_1 = sample_data(df_1, data_size)

df = pd.concat([df_1, df_0])
df.CASE_STATUS.value_counts()

x = df.drop(['CASE_STATUS'], axis=1) # Independent variables
y = df['CASE_STATUS'] # Dependent variables

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)

"""## Model Selection"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from sklearn import metrics

loreg = LogisticRegression(random_state=42, solver='lbfgs')
loreg.fit(X_train, Y_train)

knn = KNeighborsClassifier(n_neighbors=24, metric='minkowski', p=2)
knn.fit(X_train, Y_train)

svc = SVC(kernel='linear', random_state=42)
svc.fit(X_train, Y_train)

nb = GaussianNB()
nb.fit(X_train, Y_train)

dtree = DecisionTreeClassifier(criterion='entropy', random_state=42)
dtree.fit(X_train, Y_train)

randforest = RandomForestClassifier(criterion='entropy', random_state=42, n_estimators=11)
randforest.fit(X_train, Y_train)

y_pred_loreg = loreg.predict(X_test)
y_pred_knn = knn.predict(X_test)
y_pred_svc = svc.predict(X_test)
y_pred_nb = nb.predict(X_test)
y_pred_dtree = dtree.predict(X_test)
y_pred_randforest = randforest.predict(X_test)

acc_loreg = accuracy_score(Y_test, y_pred_loreg)
acc_knn = accuracy_score(Y_test, y_pred_knn)
acc_svc = accuracy_score(Y_test, y_pred_svc)
acc_nb = accuracy_score(Y_test, y_pred_nb)
acc_dtree = accuracy_score(Y_test, y_pred_dtree)
acc_randforest = accuracy_score(Y_test, y_pred_randforest)

print('Logisitic Regression: ' + str(acc_loreg * 100))
print('KNN: ' + str(acc_knn * 100))
print('SVC: ' + str(acc_svc * 100))
print('Naive Bayes: ' + str(acc_nb * 100))
print('Decision Tree: ' + str(acc_dtree * 100))
print('Random Forest: ' + str(acc_randforest * 100))

print(classification_report(Y_test, y_pred_nb))

cm = confusion_matrix(Y_test, y_pred_nb)
sns.heatmap(pd.DataFrame(cm), annot=True)